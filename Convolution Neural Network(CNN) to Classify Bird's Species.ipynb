{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87155f7",
   "metadata": {},
   "source": [
    "# Convolution Neural Network(CNN) to Classify Human's Feelings\n",
    "\n",
    "## IMPORTANT: This Project is 100% done by Max, HONG Ka Ho serving as academic purpose only\n",
    "\n",
    "Recently, I stumple upon a very interesting data set at Kaggle which is a data set of 275 bird species with 39364 training images, 1375 test images(5 per species) and 1375 validation images. To make things simplier, I would only consider 15 species only. I think it would be a great place for me to apply my knowledge of CNN to see if I can successfully build a trust-worth model to indicate the species that that specific bird belongs to. I can't wait to share my project results to you.\n",
    "\n",
    "Source of the dataset: https://www.kaggle.com/gpiosenka/100-bird-species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b9a7f",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "![pcscreenshot](image3.png \"File List\")\n",
    "![pcscreenshot](image1.png \"File List\")\n",
    "![pcscreenshot](image2.png \"File List\")\n",
    "\n",
    "We can see from the above images that all the files are being arranged regularly that are segregated into train, test and validation set which each file inside comprised 275 files, one specy one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4637ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import PIL\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import sys\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "sys.modules['Image'] = Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d910f26",
   "metadata": {},
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d24d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2202 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# we would apply transformation of image to avoid over-fitting problems\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'archive1/train',\n",
    "        target_size = (224, 224),\n",
    "        batch_size = 32,\n",
    "        class_mode = \"categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea234b",
   "metadata": {},
   "source": [
    "### Preprocessing the Validation Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3b207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 images belonging to 15 classes.\n",
      "Found 75 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'archive1/test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_set = test_datagen.flow_from_directory(\n",
    "        'archive1/valid',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd236d",
   "metadata": {},
   "source": [
    "# Build the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f78e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "69/69 [==============================] - 44s 573ms/step - loss: 2.7312 - accuracy: 0.2357 - val_loss: 1.7812 - val_accuracy: 0.3733\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 37s 534ms/step - loss: 1.5041 - accuracy: 0.5282 - val_loss: 1.0642 - val_accuracy: 0.6133\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 37s 532ms/step - loss: 1.0833 - accuracy: 0.6599 - val_loss: 0.6085 - val_accuracy: 0.7600\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 37s 530ms/step - loss: 0.9068 - accuracy: 0.7048 - val_loss: 0.6092 - val_accuracy: 0.8000\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 37s 536ms/step - loss: 0.8227 - accuracy: 0.7293 - val_loss: 0.4153 - val_accuracy: 0.8533\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 36s 520ms/step - loss: 0.6705 - accuracy: 0.7861 - val_loss: 0.4568 - val_accuracy: 0.8000\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 37s 533ms/step - loss: 0.5433 - accuracy: 0.8297 - val_loss: 0.3263 - val_accuracy: 0.9067\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 37s 537ms/step - loss: 0.5176 - accuracy: 0.8288 - val_loss: 0.3530 - val_accuracy: 0.8533\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 36s 519ms/step - loss: 0.4532 - accuracy: 0.8483 - val_loss: 0.2358 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 36s 514ms/step - loss: 0.3897 - accuracy: 0.8688 - val_loss: 0.4109 - val_accuracy: 0.8400\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 36s 519ms/step - loss: 0.3253 - accuracy: 0.8896 - val_loss: 0.2510 - val_accuracy: 0.9200\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 36s 512ms/step - loss: 0.2927 - accuracy: 0.9064 - val_loss: 0.2102 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 35s 512ms/step - loss: 0.3279 - accuracy: 0.9042 - val_loss: 0.2363 - val_accuracy: 0.9067\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 36s 517ms/step - loss: 0.2666 - accuracy: 0.9110 - val_loss: 0.2597 - val_accuracy: 0.9067\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 71s 1s/step - loss: 0.2525 - accuracy: 0.9205 - val_loss: 0.3089 - val_accuracy: 0.8667\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 53s 766ms/step - loss: 0.1933 - accuracy: 0.9396 - val_loss: 0.3107 - val_accuracy: 0.9067\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 54s 777ms/step - loss: 0.1843 - accuracy: 0.9401 - val_loss: 0.1978 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 34s 488ms/step - loss: 0.1762 - accuracy: 0.9450 - val_loss: 0.2368 - val_accuracy: 0.9200\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 36s 518ms/step - loss: 0.1775 - accuracy: 0.9423 - val_loss: 0.2090 - val_accuracy: 0.9067\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 34s 490ms/step - loss: 0.1554 - accuracy: 0.9500 - val_loss: 0.1798 - val_accuracy: 0.9467\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 34s 487ms/step - loss: 0.1434 - accuracy: 0.9532 - val_loss: 0.3416 - val_accuracy: 0.8933\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 34s 487ms/step - loss: 0.1558 - accuracy: 0.9496 - val_loss: 0.1285 - val_accuracy: 0.9600\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 36s 514ms/step - loss: 0.0883 - accuracy: 0.9687 - val_loss: 0.1019 - val_accuracy: 0.9600\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 34s 487ms/step - loss: 0.1152 - accuracy: 0.9632 - val_loss: 0.3905 - val_accuracy: 0.8533\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 34s 491ms/step - loss: 0.1293 - accuracy: 0.9541 - val_loss: 0.3131 - val_accuracy: 0.9067\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 34s 486ms/step - loss: 0.1160 - accuracy: 0.9614 - val_loss: 0.1638 - val_accuracy: 0.9467\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 34s 496ms/step - loss: 0.1007 - accuracy: 0.9650 - val_loss: 0.2219 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 33s 480ms/step - loss: 0.1145 - accuracy: 0.9623 - val_loss: 0.2440 - val_accuracy: 0.9333\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 34s 483ms/step - loss: 0.0580 - accuracy: 0.9791 - val_loss: 0.1506 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 33s 477ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 0.1729 - val_accuracy: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2af7608abe0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize cnn\n",
    "cnn = tf.keras.models.Sequential()\n",
    "# add convolution layer to cnn\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu',input_shape = (224, 224, 3), padding=\"valid\"))# Add Pooling to convolution layer\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), strides = 2))\n",
    "# Add additional convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), strides = 2))\n",
    "# flattening the convolution layers\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "# Fully connect the layers\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "# create output layer\n",
    "cnn.add(tf.keras.layers.Dense(units = 15, activation = 'softmax'))\n",
    "# compile CNN\n",
    "cnn.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "# activate early stopping to avoid overfitting\n",
    "es = EarlyStopping(monitor = 'loss', min_delta = 1e-10, patience = 10, verbose = 1)\n",
    "# reduce learning rate when learning plateau\n",
    "rlr = ReduceLROnPlateau(monitor = 'loss', factor = 0.2, patience = 5, verbose = 1)\n",
    "tb = TensorBoard(log_dir = logdir)\n",
    "# train the cnn\n",
    "cnn.fit(x = training_set, validation_data = valid_set, epochs = 30, callbacks = [rlr, es, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea76492",
   "metadata": {},
   "source": [
    "Result of the CNN after 30 epochs has reached around 99% accuracy in training set and 92% accuracy in validation set respectively which are pretty good results! Let's try to apply our model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1917aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "cnn.save('save/', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddc2b9",
   "metadata": {},
   "source": [
    "## Testing of Model\n",
    "\n",
    "In this session, we are going to choose one picture from each category and then pass them into the model to check their prediction accuracy. \n",
    "![pcscreenshot](image5.png \"File List\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc206cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "species_name = os.listdir('archive1/train')\n",
    "predicted_set = []\n",
    "m = []\n",
    "for bird_specy in species_name:\n",
    "    for i in range(1, 6):\n",
    "        test_image = image.load_img('archive1/test/{}/{}.jpg'.format(bird_specy, i), target_size = (224, 224))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = cnn.predict(test_image / 255.0)\n",
    "        max_index = np.where((result == np.amax(result))[0])[0][0]\n",
    "        predicted_species = species_name[max_index]\n",
    "        predicted_set.append(predicted_species)\n",
    "        if predicted_species == bird_specy:\n",
    "            m.append(1)\n",
    "        else:\n",
    "            m.append(0)\n",
    "accuracy = np.sum(m) / len(m)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb80b06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CALIFORNIA CONDOR',\n",
       " 'AFRICAN CROWNED CRANE',\n",
       " 'AFRICAN CROWNED CRANE',\n",
       " 'AFRICAN CROWNED CRANE',\n",
       " 'AFRICAN CROWNED CRANE',\n",
       " 'AFRICAN FIREFINCH',\n",
       " 'AFRICAN FIREFINCH',\n",
       " 'AFRICAN FIREFINCH',\n",
       " 'AFRICAN FIREFINCH',\n",
       " 'AFRICAN FIREFINCH',\n",
       " 'AMERICAN AVOCET',\n",
       " 'ALBATROSS',\n",
       " 'ALBATROSS',\n",
       " 'AMERICAN AVOCET',\n",
       " 'ALBATROSS',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'AMERICAN AVOCET',\n",
       " 'AFRICAN CROWNED CRANE',\n",
       " 'AMERICAN AVOCET',\n",
       " 'AMERICAN AVOCET',\n",
       " 'AMERICAN AVOCET',\n",
       " 'BALD EAGLE',\n",
       " 'BALD EAGLE',\n",
       " 'BALD EAGLE',\n",
       " 'BALD EAGLE',\n",
       " 'BALD EAGLE',\n",
       " 'BALI STARLING',\n",
       " 'BALI STARLING',\n",
       " 'BALI STARLING',\n",
       " 'BALI STARLING',\n",
       " 'BALI STARLING',\n",
       " 'BALTIMORE ORIOLE',\n",
       " 'BALTIMORE ORIOLE',\n",
       " 'BALTIMORE ORIOLE',\n",
       " 'BALTIMORE ORIOLE',\n",
       " 'BALTIMORE ORIOLE',\n",
       " 'BANANAQUIT',\n",
       " 'BANANAQUIT',\n",
       " 'CANARY',\n",
       " 'BANANAQUIT',\n",
       " 'BANANAQUIT',\n",
       " 'BANDED BROADBILL',\n",
       " 'BANDED BROADBILL',\n",
       " 'BANDED BROADBILL',\n",
       " 'BANDED BROADBILL',\n",
       " 'BANDED BROADBILL',\n",
       " 'CACTUS WREN',\n",
       " 'CACTUS WREN',\n",
       " 'CACTUS WREN',\n",
       " 'CACTUS WREN',\n",
       " 'CACTUS WREN',\n",
       " 'CALIFORNIA CONDOR',\n",
       " 'CALIFORNIA CONDOR',\n",
       " 'CALIFORNIA CONDOR',\n",
       " 'CALIFORNIA CONDOR',\n",
       " 'CALIFORNIA CONDOR',\n",
       " 'ALBATROSS',\n",
       " 'CALIFORNIA GULL',\n",
       " 'CALIFORNIA GULL',\n",
       " 'CALIFORNIA GULL',\n",
       " 'CALIFORNIA GULL',\n",
       " 'CALIFORNIA QUAIL',\n",
       " 'CALIFORNIA QUAIL',\n",
       " 'CALIFORNIA GULL',\n",
       " 'CALIFORNIA QUAIL',\n",
       " 'CALIFORNIA QUAIL',\n",
       " 'CANARY',\n",
       " 'ALEXANDRINE PARAKEET',\n",
       " 'CANARY',\n",
       " 'CANARY',\n",
       " 'CANARY']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the predictions\n",
    "predicted_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a33f7",
   "metadata": {},
   "source": [
    "From the above result, I am confident to tell you that my model has achieved high accuracy in the sense that it reaches around 90% accuracy. However, there are one potential problem inside that it fails to accurately classify the ALBATROSS specy of bird. With a view to that, more data or more training is needed to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf869dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
